import json
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

def create_vectorstore():
    """Create FAISS vector store from scraped data"""
    
    # 1. Load scraped data
    print("ğŸ“– Loading scraped data...")
    with open('data/scraped_data.json', 'r', encoding='utf-8') as f:
        pages = json.load(f)
    
    print(f"âœ… Loaded {len(pages)} pages")
    
    # 2. Combine all text
    print("\nğŸ“ Combining text from all pages...")
    all_text = []
    for page in pages:
        page_text = f"SOURCE: {page['url']}\nTITLE: {page['title']}\n\n{page['content']}\n\n"
        all_text.append(page_text)
    
    combined_text = "\n---\n".join(all_text)
    print(f"âœ… Combined text length: {len(combined_text):,} characters")
    
    # 3. Split into chunks
    print("\nâœ‚ï¸  Splitting into chunks...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    chunks = splitter.split_text(combined_text)
    print(f"âœ… Created {len(chunks)} chunks")
    
    # 4. Save chunks to readable file (for verification)
    print("\nğŸ’¾ Saving chunks to readable file...")
    os.makedirs('data', exist_ok=True)
    with open('data/chunks.json', 'w', encoding='utf-8') as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… Chunks saved to: data/chunks.json (you can read this!)")
    
    # 5. Create embeddings
    print("\nğŸ”§ Creating embeddings model...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    print("âœ… Embeddings model ready")
    
    # 6. Create FAISS vector store
    print("\nğŸ§  Creating FAISS vector store...")
    vectorstore = FAISS.from_texts(chunks, embeddings)
    
    # 7. Save vector store
    print("\nğŸ’¾ Saving vector store...")
    os.makedirs('vectorstore', exist_ok=True)
    vectorstore.save_local('vectorstore')
    
    # Check file sizes
    faiss_size = os.path.getsize('vectorstore/index.faiss')
    pkl_size = os.path.getsize('vectorstore/index.pkl')
    
    print(f"\nâœ… Vector store created successfully!")
    print(f"   ğŸ“ vectorstore/index.faiss: {faiss_size:,} bytes")
    print(f"   ğŸ“ vectorstore/index.pkl: {pkl_size:,} bytes")
    print(f"\nğŸ“Š Summary:")
    print(f"   - Pages scraped: {len(pages)}")
    print(f"   - Text chunks: {len(chunks)}")
    print(f"   - Vector dimensions: 384 (MiniLM)")
    
    return vectorstore

if __name__ == "__main__":
    create_vectorstore()